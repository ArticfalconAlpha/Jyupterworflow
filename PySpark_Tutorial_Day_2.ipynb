{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a197a4",
   "metadata": {},
   "source": [
    "# Day-2 of PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0652c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f604b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practice_Day_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0fb7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SalaryFile='C:\\\\Users\\\\sunny\\\\OneDrive\\\\Desktop\\\\Pyspark\\\\Files\\\\CSV\\\\Salary.csv'\n",
    "df_pyspark=spark.read.csv(SalaryFile,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f52b765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Mélina|  30|         8| 25009|\n",
      "|    Léane|  26|         5| 11377|\n",
      "|   Garçon|  29|         5| 21428|\n",
      "|      Ráo|  26|         5| 20726|\n",
      "|   Maëlys|  28|         7| 18359|\n",
      "|       Wá|  30|         8| 17206|\n",
      "|  Rébecca|  29|         7| 25289|\n",
      "|       Pò|  26|         8| 21221|\n",
      "|  Pélagie|  26|         8| 27729|\n",
      "|     Cloé|  25|         8| 19411|\n",
      "|Angélique|  27|         8| 19282|\n",
      "|    Léana|  28|         5| 22181|\n",
      "|  Océanne|null|      null| 25784|\n",
      "|     null|  27|         8| 28366|\n",
      "|     null|  30|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae8b8406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|   Mélina| 30|         8| 25009|\n",
      "|    Léane| 26|         5| 11377|\n",
      "|   Garçon| 29|         5| 21428|\n",
      "|      Ráo| 26|         5| 20726|\n",
      "|   Maëlys| 28|         7| 18359|\n",
      "|       Wá| 30|         8| 17206|\n",
      "|  Rébecca| 29|         7| 25289|\n",
      "|       Pò| 26|         8| 21221|\n",
      "|  Pélagie| 26|         8| 27729|\n",
      "|     Cloé| 25|         8| 19411|\n",
      "|Angélique| 27|         8| 19282|\n",
      "|    Léana| 28|         5| 22181|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## below command will drop any rows which have null values in any column\n",
    "## last three values were dropped\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a69f63b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Mélina|  30|         8| 25009|\n",
      "|    Léane|  26|         5| 11377|\n",
      "|   Garçon|  29|         5| 21428|\n",
      "|      Ráo|  26|         5| 20726|\n",
      "|   Maëlys|  28|         7| 18359|\n",
      "|       Wá|  30|         8| 17206|\n",
      "|  Rébecca|  29|         7| 25289|\n",
      "|       Pò|  26|         8| 21221|\n",
      "|  Pélagie|  26|         8| 27729|\n",
      "|     Cloé|  25|         8| 19411|\n",
      "|Angélique|  27|         8| 19282|\n",
      "|    Léana|  28|         5| 22181|\n",
      "|  Océanne|null|      null| 25784|\n",
      "|     null|  27|         8| 28366|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## if how='any' then it will drop any rows which have any number of null columns\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "307d48d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Mélina|  30|         8| 25009|\n",
      "|    Léane|  26|         5| 11377|\n",
      "|   Garçon|  29|         5| 21428|\n",
      "|      Ráo|  26|         5| 20726|\n",
      "|   Maëlys|  28|         7| 18359|\n",
      "|       Wá|  30|         8| 17206|\n",
      "|  Rébecca|  29|         7| 25289|\n",
      "|       Pò|  26|         8| 21221|\n",
      "|  Pélagie|  26|         8| 27729|\n",
      "|     Cloé|  25|         8| 19411|\n",
      "|Angélique|  27|         8| 19282|\n",
      "|    Léana|  28|         5| 22181|\n",
      "|  Océanne|null|      null| 25784|\n",
      "|     null|  27|         8| 28366|\n",
      "|     null|  30|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## if how='all' then it will drop any rows which have all columns NULL\n",
    "## in this case none is dropped because we have atleast 1 Non-Null Value\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09a27631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Mélina|  30|         8| 25009|\n",
      "|    Léane|  26|         5| 11377|\n",
      "|   Garçon|  29|         5| 21428|\n",
      "|      Ráo|  26|         5| 20726|\n",
      "|   Maëlys|  28|         7| 18359|\n",
      "|       Wá|  30|         8| 17206|\n",
      "|  Rébecca|  29|         7| 25289|\n",
      "|       Pò|  26|         8| 21221|\n",
      "|  Pélagie|  26|         8| 27729|\n",
      "|     Cloé|  25|         8| 19411|\n",
      "|Angélique|  27|         8| 19282|\n",
      "|    Léana|  28|         5| 22181|\n",
      "|  Océanne|null|      null| 25784|\n",
      "|     null|  27|         8| 28366|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##thresh checks if a row has atleas 2 t non null values if not that row will be deleted\n",
    "## in the below example it is checking 2 non null values because value of thresh is 2  and only last is deleted\n",
    "df_pyspark.na.drop(how='any' , thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11d0739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Mélina|  30|         8| 25009|\n",
      "|    Léane|  26|         5| 11377|\n",
      "|   Garçon|  29|         5| 21428|\n",
      "|      Ráo|  26|         5| 20726|\n",
      "|   Maëlys|  28|         7| 18359|\n",
      "|       Wá|  30|         8| 17206|\n",
      "|  Rébecca|  29|         7| 25289|\n",
      "|       Pò|  26|         8| 21221|\n",
      "|  Pélagie|  26|         8| 27729|\n",
      "|     Cloé|  25|         8| 19411|\n",
      "|Angélique|  27|         8| 19282|\n",
      "|    Léana|  28|         5| 22181|\n",
      "|  Océanne|null|      null| 25784|\n",
      "|     null|  27|         8| 28366|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## below it is checking at least 1 non null which satisfies all the rows none is deleted/dropped\n",
    "## works with both any and all\n",
    "## df_pyspark.na.drop(how='all' , thresh=2).show()\n",
    "df_pyspark.na.drop(how='any' , thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8635681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Mélina|  30|         8| 25009|\n",
      "|    Léane|  26|         5| 11377|\n",
      "|   Garçon|  29|         5| 21428|\n",
      "|      Ráo|  26|         5| 20726|\n",
      "|   Maëlys|  28|         7| 18359|\n",
      "|       Wá|  30|         8| 17206|\n",
      "|  Rébecca|  29|         7| 25289|\n",
      "|       Pò|  26|         8| 21221|\n",
      "|  Pélagie|  26|         8| 27729|\n",
      "|     Cloé|  25|         8| 19411|\n",
      "|Angélique|  27|         8| 19282|\n",
      "|    Léana|  28|         5| 22181|\n",
      "|  Océanne|null|      null| 25784|\n",
      "|     null|  27|         8| 28366|\n",
      "|     null|  30|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n",
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|   Mélina| 30|         8| 25009|\n",
      "|    Léane| 26|         5| 11377|\n",
      "|   Garçon| 29|         5| 21428|\n",
      "|      Ráo| 26|         5| 20726|\n",
      "|   Maëlys| 28|         7| 18359|\n",
      "|       Wá| 30|         8| 17206|\n",
      "|  Rébecca| 29|         7| 25289|\n",
      "|       Pò| 26|         8| 21221|\n",
      "|  Pélagie| 26|         8| 27729|\n",
      "|     Cloé| 25|         8| 19411|\n",
      "|Angélique| 27|         8| 19282|\n",
      "|    Léana| 28|         5| 22181|\n",
      "|     null| 27|         8| 28366|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SUbset will specify the columns\n",
    "df_pyspark.show()\n",
    "df_pyspark.na.drop(how='any' , subset='Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2173057",
   "metadata": {},
   "outputs": [],
   "source": [
    "## thresh doesn't work with subset(Need to Confirm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b19a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|   Mélina|  30|         8| 25009|\n",
      "|    Léane|  26|         5| 11377|\n",
      "|   Garçon|  29|         5| 21428|\n",
      "|      Ráo|  26|         5| 20726|\n",
      "|   Maëlys|  28|         7| 18359|\n",
      "|       Wá|  30|         8| 17206|\n",
      "|  Rébecca|  29|         7| 25289|\n",
      "|       Pò|  26|         8| 21221|\n",
      "|  Pélagie|  26|         8| 27729|\n",
      "|     Cloé|  25|         8| 19411|\n",
      "|Angélique|  27|         8| 19282|\n",
      "|    Léana|  28|         5| 22181|\n",
      "|  Océanne|null|      null| 25784|\n",
      "|  Missing|  27|         8| 28366|\n",
      "|  Missing|  30|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filling the values\n",
    "df_pyspark.na.fill('Missing' ,['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de209064",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imputer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\__init__.py:22\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one or more\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# contributor license agreements.  See the NOTICE file distributed with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mDataFrame-based machine learning APIs to let users quickly assemble and configure practical\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mmachine learning pipelines.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[0;32m     25\u001b[0m     Predictor,\n\u001b[0;32m     26\u001b[0m     PredictionModel,\n\u001b[0;32m     27\u001b[0m     Transformer,\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:40\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     Any,\n\u001b[0;32m     25\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m since\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m P\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inherit_doc\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     HasInputCol,\n\u001b[0;32m     44\u001b[0m     HasOutputCol,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     Params,\n\u001b[0;32m     49\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\param\\__init__.py:32\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     Any,\n\u001b[0;32m     22\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JavaObject\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseVector, Vector, Matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4e636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
